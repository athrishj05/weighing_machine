{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqJWZ8vIneBq",
        "outputId": "530328c5-206a-4fa6-f2e7-f10c625a6787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file âœ… \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0% Complete"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0% CompleteFound 162 images in /content\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8n.pt to 'yolov8n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 6.2MB 119.8MB/s 0.1s\n",
            "Ultralytics 8.4.9 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/dataset/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, end2end=None, epochs=25, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 23.5MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, 16, None, [64, 128, 256]] \n",
            "Model summary: 130 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n.pt to 'yolo26n.pt': 100% â”â”â”â”â”â”â”â”â”â”â”â” 5.3MB 106.5MB/s 0.0s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2141.2Â±492.2 MB/s, size: 186.6 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/dataset/labels/train... 112 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 112/112 1.6Kit/s 0.1s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/dataset/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 581.8Â±67.1 MB/s, size: 146.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset/labels/val... 31 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 31/31 1.1Kit/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/dataset/labels/val.cache\n",
            "Plotting labels to /content/runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/train\u001b[0m\n",
            "Starting training for 25 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/25      4.84G      2.742      3.743      2.402         46        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 3.7it/s 3.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 1.5it/s 1.4s\n",
            "                   all         31        107    0.00495       0.43     0.0437    0.00997\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/25      4.86G      2.441      2.836      2.082         67        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 6.7it/s 2.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 8.9it/s 0.2s\n",
            "                   all         31        107    0.00538      0.467     0.0459      0.015\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/25      4.87G      2.338      2.482      1.935         47        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 4.6it/s 3.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 9.7it/s 0.2s\n",
            "                   all         31        107      0.719     0.0245     0.0804     0.0287\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/25      4.89G      2.164       2.35      1.848         40        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 6.6it/s 2.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 8.8it/s 0.2s\n",
            "                   all         31        107      0.344      0.128     0.0856     0.0287\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/25      4.91G      2.141      2.221      1.846         41        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 6.9it/s 2.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 7.3it/s 0.3s\n",
            "                   all         31        107      0.315     0.0991     0.0873     0.0263\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/25      4.92G      2.173      2.206      1.805         52        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 6.7it/s 2.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 7.9it/s 0.3s\n",
            "                   all         31        107      0.257     0.0935     0.0898     0.0293\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/25      4.94G       2.17      2.309      1.781         44        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 4.7it/s 3.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.9it/s 0.4s\n",
            "                   all         31        107      0.292      0.187      0.152     0.0465\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/25      4.96G      2.035      2.227      1.704         37        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 6.6it/s 2.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 7.3it/s 0.3s\n",
            "                   all         31        107       0.33      0.327       0.21     0.0711\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/25      4.98G      1.992      2.114      1.683         80        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 6.8it/s 2.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 5.2it/s 0.4s\n",
            "                   all         31        107      0.496      0.411      0.321      0.122\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/25      4.99G      1.967      2.128      1.681         35        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 6.6it/s 2.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 7.1it/s 0.3s\n",
            "                   all         31        107      0.416      0.364       0.27     0.0911\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      11/25      5.01G      1.936      1.978      1.678         31        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 6.3it/s 2.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.2it/s 0.5s\n",
            "                   all         31        107      0.291      0.411      0.243     0.0832\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      12/25      5.03G      1.902      2.021      1.628         60        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 5.5it/s 2.5s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 8.5it/s 0.2s\n",
            "                   all         31        107      0.496      0.486      0.431      0.158\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      13/25      5.04G      1.851      1.966      1.617         44        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 6.8it/s 2.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 7.0it/s 0.3s\n",
            "                   all         31        107       0.51      0.533       0.41      0.165\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      14/25      5.06G      1.859       1.94      1.617         56        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 7.2it/s 1.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 6.2it/s 0.3s\n",
            "                   all         31        107      0.413      0.473      0.368       0.16\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      15/25      5.08G      1.868      1.911      1.619         60        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 7.3it/s 1.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 5.0it/s 0.4s\n",
            "                   all         31        107      0.376      0.541      0.392      0.172\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      16/25       5.1G      1.771        2.1      1.641         22        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 3.3it/s 4.2s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.0it/s 0.5s\n",
            "                   all         31        107      0.496      0.561       0.42      0.176\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      17/25      5.11G      1.714      2.069      1.605         25        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 6.9it/s 2.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 6.3it/s 0.3s\n",
            "                   all         31        107      0.551       0.55      0.451      0.184\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      18/25      5.13G      1.699      2.043      1.626         20        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 7.0it/s 2.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 10.1it/s 0.2s\n",
            "                   all         31        107      0.639      0.545      0.488      0.212\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      19/25      5.15G       1.66      1.986      1.596         24        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 6.7it/s 2.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.2it/s 0.5s\n",
            "                   all         31        107       0.59      0.533      0.489      0.222\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      20/25      5.16G      1.721      2.018      1.662         26        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 5.0it/s 2.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 6.7it/s 0.3s\n",
            "                   all         31        107      0.431      0.598      0.466      0.237\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      21/25      5.18G      1.653      1.951      1.583         17        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 6.8it/s 2.1s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 7.5it/s 0.3s\n",
            "                   all         31        107      0.464      0.579      0.477      0.235\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      22/25       5.2G      1.696      1.931      1.592         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 7.1it/s 2.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 7.6it/s 0.3s\n",
            "                   all         31        107      0.532      0.561      0.524       0.25\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      23/25      5.22G      1.588      1.912      1.527         29        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 7.3it/s 1.9s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 5.4it/s 0.4s\n",
            "                   all         31        107      0.691      0.458      0.531      0.255\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      24/25      5.24G       1.57      1.872      1.509         30        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 5.0it/s 2.8s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 4.7it/s 0.4s\n",
            "                   all         31        107      0.647      0.496      0.535      0.262\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      25/25      5.25G      1.563      1.888      1.544         18        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 14/14 7.0it/s 2.0s\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 7.8it/s 0.3s\n",
            "                   all         31        107      0.632      0.533      0.543      0.271\n",
            "\n",
            "25 epochs completed in 0.021 hours.\n",
            "Optimizer stripped from /content/runs/detect/train/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from /content/runs/detect/train/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating /content/runs/detect/train/weights/best.pt...\n",
            "Ultralytics 8.4.9 ğŸš€ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Model summary (fused): 73 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 2/2 5.0it/s 0.4s\n",
            "                   all         31        107      0.631      0.533      0.543      0.271\n",
            "Speed: 0.2ms preprocess, 1.6ms inference, 0.0ms loss, 3.4ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/detect/train\u001b[0m\n",
            "Pipeline completed. Check dataset and inference_results folders.\n"
          ]
        }
      ],
      "source": [
        "# AutoScale Pipeline - single-file runnable in Colab\n",
        "# Features:\n",
        "# - Mount Google Drive (optional)\n",
        "# - Collect images from a folder\n",
        "# - Heuristic auto-labeling (EasyOCR + contour expansion) when labels missing\n",
        "# - Create YOLOv8 dataset structure and split\n",
        "# - Train YOLOv8 (yolov8n) and save best weights\n",
        "# - Run inference on test set and save annotated images + CSV of confidences\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import random\n",
        "import glob\n",
        "import csv\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# --------- Configuration ---------\n",
        "SOURCE_DIR = \"/content\"  # change to your path (after mounting Drive if needed)\n",
        "OUTPUT_DATASET = \"/content/dataset\"\n",
        "AUTO_LABEL_DIR = \"/content/auto_labels\"\n",
        "IMAGE_EXTS = (\".jpg\", \".jpeg\", \".png\")\n",
        "CLASS_NAME = \"weighing_scale\"\n",
        "SEED = 42\n",
        "SPLIT = {\"train\": 0.7, \"val\": 0.2, \"test\": 0.1}\n",
        "YOLO_EPOCHS = 25\n",
        "IMG_SIZE = 640\n",
        "BATCH = 8\n",
        "\n",
        "# --------- Helpers ---------\n",
        "def pip_install(pkgs: list):\n",
        "    import subprocess\n",
        "    for p in pkgs:\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", p], check=True)\n",
        "\n",
        "# Install required packages if missing\n",
        "try:\n",
        "    import easyocr\n",
        "except Exception:\n",
        "    pip_install([\"easyocr\",\"ultralytics\",\"opencv-python\",\"numpy\"])\n",
        "    import easyocr\n",
        "    from ultralytics import YOLO\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import easyocr\n",
        "import torch\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(AUTO_LABEL_DIR, exist_ok=True)\n",
        "os.makedirs(OUTPUT_DATASET, exist_ok=True)\n",
        "\n",
        "# --------- Auto-labeling Routine (OCR + Contour heuristic) ---------\n",
        "# Fixed: Using torch.cuda.is_available() for more reliable GPU check in Colab\n",
        "reader = easyocr.Reader([\"en\"], gpu=torch.cuda.is_available())\n",
        "\n",
        "def find_display_box_by_ocr(img):\n",
        "    # returns list of boxes in xyxy (x1,y1,x2,y2)\n",
        "    h, w = img.shape[:2]\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    # apply OCR\n",
        "    try:\n",
        "        results = reader.readtext(img)\n",
        "    except Exception:\n",
        "        results = []\n",
        "    boxes = []\n",
        "    for (bbox, text, conf) in results:\n",
        "        if len(text.strip()) == 0:\n",
        "            continue\n",
        "        if any(ch.isdigit() for ch in text):\n",
        "            xs = [int(max(0, min(w-1, p[0]))) for p in bbox]\n",
        "            ys = [int(max(0, min(h-1, p[1]))) for p in bbox]\n",
        "            x1, x2 = min(xs), max(xs)\n",
        "            y1, y2 = min(ys), max(ys)\n",
        "            pad_x = int((x2-x1) * 0.6)\n",
        "            pad_y = int((y2-y1) * 0.6)\n",
        "            x1 = max(0, x1 - pad_x)\n",
        "            x2 = min(w-1, x2 + pad_x)\n",
        "            y1 = max(0, y1 - pad_y)\n",
        "            y2 = min(h-1, y2 + pad_y)\n",
        "            boxes.append((x1,y1,x2,y2))\n",
        "    return boxes\n",
        "\n",
        "def find_display_box_by_contour(img):\n",
        "    h, w = img.shape[:2]\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    blur = cv2.GaussianBlur(gray, (5,5), 0)\n",
        "    edged = cv2.Canny(blur, 50, 150)\n",
        "    cnts, _ = cv2.findContours(edged, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    candidates = []\n",
        "    for c in cnts:\n",
        "        area = cv2.contourArea(c)\n",
        "        if area < (w*h)*0.001:\n",
        "            continue\n",
        "        peri = cv2.arcLength(c, True)\n",
        "        approx = cv2.approxPolyDP(c, 0.02*peri, True)\n",
        "        x,y,ww,hh = cv2.boundingRect(approx)\n",
        "        aspect = ww/float(hh+1e-6)\n",
        "        if 0.6 < aspect < 4.0:\n",
        "            candidates.append((area, x,y,x+ww,y+hh))\n",
        "    if not candidates:\n",
        "        return []\n",
        "    candidates.sort(reverse=True, key=lambda x: x[0])\n",
        "    _, x1,y1,x2,y2 = candidates[0]\n",
        "    pad_x = int((x2-x1)*0.25)\n",
        "    pad_y = int((y2-y1)*0.25)\n",
        "    x1 = max(0, x1-pad_x); x2=min(w-1, x2+pad_x)\n",
        "    y1 = max(0, y1-pad_y); y2=min(h-1, y2+pad_y)\n",
        "    return [(x1,y1,x2,y2)]\n",
        "\n",
        "def img_to_yolo_label(img_path, boxes, out_label_path):\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None: return\n",
        "    h,w = img.shape[:2]\n",
        "    with open(out_label_path, \"w\") as f:\n",
        "        for (x1,y1,x2,y2) in boxes:\n",
        "            xc = ((x1 + x2)/2.0) / w\n",
        "            yc = ((y1 + y2)/2.0) / h\n",
        "            bw = (x2 - x1) / w\n",
        "            bh = (y2 - y1) / h\n",
        "            f.write(f\"0 {xc:.6f} {yc:.6f} {bw:.6f} {bh:.6f}\\n\")\n",
        "\n",
        "# Process images: create labels if missing\n",
        "images = [p for p in Path(SOURCE_DIR).glob(\"**/*\") if p.suffix.lower() in IMAGE_EXTS]\n",
        "print(f\"Found {len(images)} images in {SOURCE_DIR}\")\n",
        "\n",
        "for p in images:\n",
        "    img_path = str(p)\n",
        "    base = p.stem\n",
        "    label_path = os.path.join(AUTO_LABEL_DIR, base + \".txt\")\n",
        "    if os.path.exists(label_path):\n",
        "        continue\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None: continue\n",
        "    boxes = find_display_box_by_ocr(img)\n",
        "    if len(boxes) == 0:\n",
        "        boxes = find_display_box_by_contour(img)\n",
        "    if len(boxes) == 0:\n",
        "        continue\n",
        "    img_to_yolo_label(img_path, boxes, label_path)\n",
        "\n",
        "# --------- Create dataset structure and split ---------\n",
        "random.seed(SEED)\n",
        "all_imgs = [p for p in Path(SOURCE_DIR).glob(\"**/*\") if p.suffix.lower() in IMAGE_EXTS]\n",
        "all_imgs = sorted([str(p) for p in all_imgs])\n",
        "random.shuffle(all_imgs)\n",
        "\n",
        "n = len(all_imgs)\n",
        "if n == 0:\n",
        "    raise SystemExit(\"No images found in SOURCE_DIR. Mount Drive or set SOURCE_DIR correctly.\")\n",
        "\n",
        "n_train = int(SPLIT['train'] * n)\n",
        "n_val = int(SPLIT['val'] * n)\n",
        "\n",
        "train_list = all_imgs[:n_train]\n",
        "val_list = all_imgs[n_train:n_train+n_val]\n",
        "test_list = all_imgs[n_train+n_val:]\n",
        "\n",
        "for split_name, lst in zip([\"train\",\"val\",\"test\"],[train_list,val_list,test_list]):\n",
        "    img_out_dir = os.path.join(OUTPUT_DATASET, \"images\", split_name)\n",
        "    lbl_out_dir = os.path.join(OUTPUT_DATASET, \"labels\", split_name)\n",
        "    os.makedirs(img_out_dir, exist_ok=True)\n",
        "    os.makedirs(lbl_out_dir, exist_ok=True)\n",
        "    for img_path in lst:\n",
        "        fname = os.path.basename(img_path)\n",
        "        base = Path(img_path).stem\n",
        "        src_label = os.path.join(AUTO_LABEL_DIR, base + \".txt\")\n",
        "        if not os.path.exists(src_label):\n",
        "            continue\n",
        "        shutil.copy(img_path, os.path.join(img_out_dir, fname))\n",
        "        shutil.copy(src_label, os.path.join(lbl_out_dir, base + \".txt\"))\n",
        "\n",
        "data_yaml = os.path.join(OUTPUT_DATASET, \"data.yaml\")\n",
        "with open(data_yaml, \"w\") as f:\n",
        "    f.write(f\"path: {OUTPUT_DATASET}\\n\")\n",
        "    f.write(\"train: images/train\\n\")\n",
        "    f.write(\"val: images/val\\n\")\n",
        "    f.write(\"test: images/test\\n\\n\")\n",
        "    f.write(\"names:\\n\")\n",
        "    f.write(f\"  0: {CLASS_NAME}\\n\")\n",
        "\n",
        "# --------- Train YOLOv8 ---------\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "model.train(data=data_yaml, epochs=YOLO_EPOCHS, imgsz=IMG_SIZE, batch=BATCH)\n",
        "\n",
        "# --------- Inference on test set ---------\n",
        "weights = None\n",
        "runs_dir = \"runs/detect\"\n",
        "possible = sorted(glob.glob(\"runs/detect/train/weights/*.pt\"), key=os.path.getmtime)\n",
        "if not possible:\n",
        "    possible = sorted(glob.glob(\"runs/detect/*/weights/*.pt\"), key=os.path.getmtime)\n",
        "if possible:\n",
        "    weights = possible[-1]\n",
        "\n",
        "if weights is None:\n",
        "    print(\"No weights found - aborting inference\")\n",
        "else:\n",
        "    model = YOLO(weights)\n",
        "    out_dir = \"/content/inference_results\"\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    csv_rows = [(\"image\",\"box_x1\",\"box_y1\",\"box_x2\",\"box_y2\",\"conf\")]\n",
        "    test_imgs = list(Path(OUTPUT_DATASET, \"images\", \"test\").glob(\"*.jpg\"))\n",
        "    for t in test_imgs:\n",
        "        res = model.predict(source=str(t), conf=0.25, save=False)\n",
        "        if len(res) > 0:\n",
        "            boxes = res[0].boxes\n",
        "            img = cv2.imread(str(t))\n",
        "            for b in boxes:\n",
        "                xyxy = b.xyxy.cpu().numpy().tolist()[0]\n",
        "                conf = float(b.conf.cpu().numpy())\n",
        "                x1,y1,x2,y2 = map(int, xyxy)\n",
        "                cv2.rectangle(img, (x1,y1),(x2,y2),(0,255,0),2)\n",
        "                cv2.putText(img, f\"{conf:.2f}\", (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0),2)\n",
        "                csv_rows.append((str(t.name), x1,y1,x2,y2,conf))\n",
        "            cv2.imwrite(os.path.join(out_dir, t.name), img)\n",
        "    with open(os.path.join(out_dir, \"detections.csv\"), \"w\", newline=\"\") as cf:\n",
        "        writer = csv.writer(cf)\n",
        "        writer.writerows(csv_rows)\n",
        "\n",
        "print(\"Pipeline completed. Check dataset and inference_results folders.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97eMDvc6uYtJ",
        "outputId": "f54ff929-0206-4b30-f984-217aadae2026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "image 1/17 /content/dataset/images/test/105.jpeg: 640x480 3 weighing_scales, 37.7ms\n",
            "image 2/17 /content/dataset/images/test/106.jpeg: 640x384 4 weighing_scales, 41.1ms\n",
            "image 3/17 /content/dataset/images/test/107.jpeg: 640x480 4 weighing_scales, 7.6ms\n",
            "image 4/17 /content/dataset/images/test/12.jpeg: 640x384 5 weighing_scales, 6.5ms\n",
            "image 5/17 /content/dataset/images/test/120.jpeg: 640x384 5 weighing_scales, 5.7ms\n",
            "image 6/17 /content/dataset/images/test/123.jpeg: 640x384 4 weighing_scales, 6.0ms\n",
            "image 7/17 /content/dataset/images/test/13.jpeg: 640x384 1 weighing_scale, 5.8ms\n",
            "image 8/17 /content/dataset/images/test/145.jpeg: 640x384 5 weighing_scales, 6.2ms\n",
            "image 9/17 /content/dataset/images/test/163.jpeg: 640x480 6 weighing_scales, 6.1ms\n",
            "image 10/17 /content/dataset/images/test/165.jpeg: 640x384 5 weighing_scales, 6.2ms\n",
            "image 11/17 /content/dataset/images/test/167.jpeg: 640x384 4 weighing_scales, 7.3ms\n",
            "image 12/17 /content/dataset/images/test/17.jpeg: 640x384 4 weighing_scales, 5.7ms\n",
            "image 13/17 /content/dataset/images/test/177.jpeg: 640x384 7 weighing_scales, 5.7ms\n",
            "image 14/17 /content/dataset/images/test/50.jpeg: 640x384 4 weighing_scales, 5.7ms\n",
            "image 15/17 /content/dataset/images/test/7.jpeg: 640x384 4 weighing_scales, 6.4ms\n",
            "image 16/17 /content/dataset/images/test/79.jpeg: 640x480 3 weighing_scales, 6.4ms\n",
            "image 17/17 /content/dataset/images/test/9.jpeg: 640x384 6 weighing_scales, 6.1ms\n",
            "Speed: 2.2ms preprocess, 10.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
            "Results saved to \u001b[1m/content/inference_results_fixed/predictions\u001b[0m\n",
            "17 labels saved to /content/inference_results_fixed/predictions/labels\n",
            "Inference completed.\n",
            "Saved to: /content/inference_results_fixed/predictions\n"
          ]
        }
      ],
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Load best model\n",
        "model = YOLO(\"/content/runs/detect/train/weights/best.pt\")\n",
        "\n",
        "TEST_DIR = \"/content/dataset/images/test\"\n",
        "OUT_DIR = \"/content/inference_results_fixed\"\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# Run inference with YOLO saving enabled\n",
        "results = model.predict(\n",
        "    source=TEST_DIR,\n",
        "    conf=0.15,          # LOWER threshold\n",
        "    iou=0.5,\n",
        "    save=True,\n",
        "    save_txt=True,\n",
        "    project=OUT_DIR,\n",
        "    name=\"predictions\"\n",
        ")\n",
        "\n",
        "print(\"Inference completed.\")\n",
        "print(\"Saved to:\", f\"{OUT_DIR}/predictions\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
